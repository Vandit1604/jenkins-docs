{"componentChunkName":"component---src-templates-article-js","path":"/2023/07/10/2023-07-12-jenkins-mirrors-postmortem-outage/","result":{"data":{"asciidoc":{"html":"<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>mirrors</p>\n</li>\n<li>\n<p>jenkins</p>\n</li>\n<li>\n<p>outage</p>\n</li>\n<li>\n<p>postmortem\nauthors:</p>\n</li>\n<li>\n<p>dduportal\nopengraph:\n  image: /images/logos/fire/fire.svg\nlinks:\ndiscourse: true\n---</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>On Friday 7th of July 2023, the Jenkins infrastructure suffered a major outage from 11:05am UTC until 15:25pm UTC with complete downtime of the following public services:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>accounts.jenkins.io</p>\n</li>\n<li>\n<p>fallback.get.jenkins.io</p>\n</li>\n<li>\n<p>get.jenkins.io</p>\n</li>\n<li>\n<p>incrementals.jenkins.io</p>\n</li>\n<li>\n<p>javadoc.jenkins.io</p>\n</li>\n<li>\n<p>plugin-health.jenkins.io</p>\n</li>\n<li>\n<p>plugin-site-issues.jenkins.io</p>\n</li>\n<li>\n<p>plugins.origin.jenkins.io</p>\n</li>\n<li>\n<p>plugins.jenkins.io</p>\n</li>\n<li>\n<p>rating.jenkins.io</p>\n</li>\n<li>\n<p>repo.azure.jenkins.io</p>\n</li>\n<li>\n<p>reports.jenkins.io</p>\n</li>\n<li>\n<p>stories.jenkins.io</p>\n</li>\n<li>\n<p>uplink.jenkins.io</p>\n</li>\n<li>\n<p>weekly.ci.jenkins.io</p>\n</li>\n<li>\n<p>www.origin.jenkins.io</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>We also had complete downtime of the following non-public services:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>ldap.jenkins.io</p>\n</li>\n<li>\n<p>previews of *.jenkins.io pull requests (infra.ci.jenkins.io)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>In addition, there was disruption (partial or complete) of the following services:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>ci.jenkins.io</p>\n</li>\n<li>\n<p>infra.ci.jenkins.io</p>\n</li>\n<li>\n<p>issues.jenkins.io</p>\n</li>\n<li>\n<p>plugins.jenkins.io</p>\n</li>\n<li>\n<p>repo.jenkins-ci.org</p>\n</li>\n<li>\n<p>www.jenkins.io</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The public IPs of these services changed (DNS records included) to:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>20.7.178.24</code> (IPv4)</p>\n</li>\n<li>\n<p><code>2603:1030:408:5::15a</code> (IPv6)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>⚠️ Update your corporate networks (DNS, proxies, firewall) if you need to access these services.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_incident_timeline\">Incident Timeline</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>10:30am UTC:</strong> After a successful upgrade of the public Kubernetes cluster in Azure to 1.25 (as part of <a href=\"https://github.com/jenkins-infra/helpdesk/issues/3582\">help desk ticket 3582</a>), we realized that the LDAP service was not reachable by the services running inside the cluster (such as accounts.jenkins.io).\nWe quickly identified IP restrictions blocking these requests as the pod originating IP was in a different range than before.</p>\n</li>\n<li>\n<p><strong>10:55am UTC:</strong> The fix (<a href=\"https://github.com/jenkins-infra/azure/pull/431\">Azure PR 431</a>) is deployed to specify a proper set of IP ranges for the pods.\nIt removed all of the node pools (all the virtual machines where the container was running) and failed to re-create them, causing a full outage of all the services running in this cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>accounts.jenkins.io</p>\n</li>\n<li>\n<p>get.jenkins.io</p>\n</li>\n<li>\n<p>incrementals.jenkins.io</p>\n</li>\n<li>\n<p>javadoc.jenkins.io</p>\n</li>\n<li>\n<p>jenkins-wiki-exporter.jenkins.io</p>\n</li>\n<li>\n<p>ldap.jenkins.io</p>\n</li>\n<li>\n<p>plugins.jenkins.io</p>\n</li>\n<li>\n<p>previews of *.jenkins.io pull requests (infra.ci.jenkins.io)</p>\n</li>\n<li>\n<p>release.ci.jenkins.io</p>\n</li>\n<li>\n<p>repo.azure.jenkins.io</p>\n</li>\n<li>\n<p>reports.jenkins.io</p>\n</li>\n<li>\n<p>stories.jenkins.io</p>\n</li>\n<li>\n<p>uplink.jenkins.io</p>\n</li>\n<li>\n<p>www.jenkins.io</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><strong>11:16am until 13:16pm UTC:</strong> The failure to re-create resources led us to spend the 2 next hours creating the cluster from scratch with a fixed network setup.</p>\n</li>\n<li>\n<p><strong>15:17pm UTC:</strong> <a href=\"https://github.com/jenkins-infra/azure/pull/432\">This pull request</a> is pushed to persist the manual work we did to recreate the cluster including the IP setup.</p>\n</li>\n<li>\n<p><strong>15:25pm UTC:</strong> All services are back to normal</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_what_happened\">What Happened?</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>When the cluster was initially created, we selected the <code>10.<strong>244</strong>.0.0/<strong>16</strong></code> virtual network IP range (ref. <a href=\"https://github.com/jenkins-infra/azure-net/blob/fcb010a5d9f164203c9a896fcb974df4051c321d/vnets.tf#L66\">Azure VNets</a>) with a <code>10.245.0.0/24</code> sub-network (ref. <a href=\"https://github.com/jenkins-infra/azure-net/blob/fcb010a5d9f164203c9a896fcb974df4051c321d/vnets.tf#L161)\">Azure subnet</a>).</p>\n</li>\n<li>\n<p>But we ignored that the <code>10.<strong>244</strong>.0.0/<strong>24</strong></code> range is the default CIDR for the Kubernetes Pods network in Azure when using the <a href=\"https://learn.microsoft.com/en-us/azure/aks/configure-kubenet\">\"kubenet\" network to support IPv6 instead of the default CNI</a>.</p>\n</li>\n<li>\n<p>The node pool re-creation failed because we assumed both ranges were able to communicate and tried to deploy an invalid setup.</p>\n</li>\n<li>\n<p>As soon as we specified a custom Pod CIDR in a distinct range, everything went fine.</p>\n</li>\n<li>\n<p>When the original cluster was deleted it transitively removed the current Public IPs, as it removed the <a href=\"https://learn.microsoft.com/en-us/azure/aks/faq#why-are-two-resource-groups-created-with-aks\">Nodes Resource Group</a> containing the Public IP.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>These public IPs should change as little as possible to avoid problems with our users running behind a corporate firewall with an allow-list.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_what_can_we_do_to_improve\">What can we do to improve?</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>As per <a href=\"https://github.com/jenkins-infra/helpdesk/issues/3582#issuecomment-1629210833\">our initial assessment</a>: protect the Public IPs from deletion by adding a <a href=\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json\">Management Lock</a>.</p>\n</li>\n<li>\n<p>As <a href=\"https://github.com/jenkins-infra/helpdesk/issues/3582#issuecomment-1629752851\">recommended by other contributors</a>: storing the Public IP in a distinct Resource Group and set up the Kubernetes-managed Load Balancers accordingly (annotation <code>service.beta.kubernetes.io/azure-load-balancer-resource-group</code>).</p>\n</li>\n<li>\n<p>Improve our network diagrams and documentation to have better access to the representation and potential overlaps when preparing operations.</p>\n</li>\n<li>\n<p>Avoid changing AKS node pools configurations all at once: we would have caught the issue after the first node pool and could have avoided a full outage (we are working on this topic for the <code>arm64</code> node pools in <a href=\"https://github.com/jenkins-infra/helpdesk/issues/3623\">PR-3623</a>).</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_from_0_to_production_in_less_than_4_hours\">From 0 to production in less than 4 hours!</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>One of the takeaways of this outage, is that we are able to recover from a full destruction of the cluster hosting almost all public services in less than <strong>4</strong> hours.</p>\n</div>\n<div class=\"paragraph\">\n<p>It&#8217;s a huge collaborative work which allowed this: from defining the architecture, building the infrastructure, backing-up its data, etc.</p>\n</div>\n<div class=\"paragraph\">\n<p>This huge effort started years ago by <a href=\"/blog/authors/rtyler/\">R. Tyler Croy</a>, <a href=\"/blog/authors/olblak/\">Olivier Vernin</a> and backed by a lot of contributors such as <a href=\"/blog/authors/daniel-beck/\">Daniel Beck</a>, <a href=\"/blog/authors/hlemeur/\">Hervé Le Meur</a>, <a href=\"/blog/authors/timja/\">Tim Jacomb</a>, <a href=\"/blog/authors/markewaite/\">Mark E Waite</a>, <a href=\"/blog/authors/smerle33/\">Stéphane Merle</a> and many more.</p>\n</div>\n<div class=\"paragraph\">\n<p>As current Infrastructure Officer, I want to thank them all so that our life is easier when catastrophic events happens!</p>\n</div>\n</div>\n</div>","document":{"title":"Post Mortem of the 7th July 2023 Jenkins Infrastructure Outage","main":"Post Mortem of the 7th July 2023 Jenkins Infrastructure Outage"},"author":{"fullName":"tags:"}}},"pageContext":{"id":"a05cc58f-298b-56b2-b0fb-c273b0908dc2"}},"staticQueryHashes":[],"slicesMap":{}}